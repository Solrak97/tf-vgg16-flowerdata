{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Solrak97/tf-vgg16-flowerdata/blob/main/VGG_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUk44wlBUSqO"
      },
      "source": [
        "# Transferencia de aprendizaje utilizando VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xj5_bnO3VsAq"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! rm -rf flowers\n",
        "! kaggle datasets download alxmamaev/flowers-recognition\n",
        "! unzip flowers-recognition.zip\n",
        "! rm -r flowers-recognition.zip\n",
        "! rm -rf flowers/sample_data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zf79yxuCRcO-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torchvision import datasets, transforms, models\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import Subset\n",
        "import pickle\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyOqFme5YBwM"
      },
      "source": [
        "## Carga del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eRkJOnpFSA7i"
      },
      "outputs": [],
      "source": [
        "transform_train = transforms.Compose([transforms.Resize((224,224)),\n",
        "                                      transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n",
        "                                      transforms.ColorJitter(brightness=1, contrast=1, saturation=1),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                               ])\n",
        "\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize((224,224)),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                               ])\n",
        "\n",
        "dataset = ImageFolder(\"flowers\")\n",
        "train, val = torch.utils.data.random_split(dataset, [4317 - 863, 863], generator=torch.Generator().manual_seed(42))\n",
        "\n",
        "train.dataset.transform = transform_train\n",
        "val.dataset.transform = transform\n",
        "\n",
        "training_loader = torch.utils.data.DataLoader(train, batch_size=200, shuffle=True)\n",
        "validation_loader = torch.utils.data.DataLoader(val, batch_size = 200, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ip5eZ-6OSVsg"
      },
      "outputs": [],
      "source": [
        "def im_convert(tensor):\n",
        "  image = tensor.cpu().clone().detach().numpy()\n",
        "  image = image.transpose(1, 2, 0)\n",
        "  image = image * np.array((0.5, 0.5, 0.5)) + np.array((0.5, 0.5, 0.5))\n",
        "  image = image.clip(0, 1)\n",
        "  return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywZj89_xTyUk"
      },
      "source": [
        "## Modificación del modelo base VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4QtN8tcYSZCo"
      },
      "outputs": [],
      "source": [
        "classes = ('margaritas', 'rosas', 'dientes de león', 'tulipanes', 'girasoles')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQuEWoCyScV_",
        "outputId": "421afc0a-6c78-446e-e007-93fe2589b6c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# Creación del modelo VGG16\n",
        "model = models.vgg16(pretrained=True)\n",
        "\n",
        "# Congelamiento del modelo base\n",
        "for param in model.features.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "n_inputs = model.classifier[6].in_features\n",
        "last_layer = nn.Linear(n_inputs, len(classes))\n",
        "\n",
        "# Reemplzo de la capa de salida\n",
        "model.classifier[6] = last_layer\n",
        "#model.classifier.append(nn.Softmax())\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW2OLHbEbKBG"
      },
      "source": [
        "## Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5isVa-FPbLzX"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYHw0ViGbYsT",
        "outputId": "85a027f5-f79c-4013-b87c-e46ecd8a08f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 1\n",
            "training loss: 0.0040, acc 0.7131 \n",
            "validation loss: 0.0025, validation acc 0.8366 \n",
            "epoch : 2\n",
            "training loss: 0.0014, acc 0.9062 \n",
            "validation loss: 0.0021, validation acc 0.8563 \n",
            "epoch : 3\n",
            "training loss: 0.0006, acc 0.9647 \n",
            "validation loss: 0.0022, validation acc 0.8702 \n",
            "epoch : 4\n",
            "training loss: 0.0003, acc 0.9858 \n",
            "validation loss: 0.0023, validation acc 0.8737 \n",
            "epoch : 5\n",
            "training loss: 0.0002, acc 0.9933 \n",
            "validation loss: 0.0023, validation acc 0.8749 \n",
            "epoch : 6\n",
            "training loss: 0.0001, acc 0.9977 \n",
            "validation loss: 0.0026, validation acc 0.8598 \n",
            "epoch : 7\n",
            "training loss: 0.0001, acc 0.9986 \n",
            "validation loss: 0.0024, validation acc 0.8714 \n",
            "epoch : 8\n",
            "training loss: 0.0000, acc 0.9991 \n",
            "validation loss: 0.0029, validation acc 0.8691 \n",
            "epoch : 9\n",
            "training loss: 0.0000, acc 0.9988 \n",
            "validation loss: 0.0026, validation acc 0.8737 \n",
            "epoch : 10\n",
            "training loss: 0.0000, acc 0.9994 \n",
            "validation loss: 0.0028, validation acc 0.8656 \n"
          ]
        }
      ],
      "source": [
        "epochs = 10\n",
        "running_loss_history = []\n",
        "running_corrects_history = []\n",
        "val_running_loss_history = []\n",
        "val_running_corrects_history = []\n",
        "\n",
        "for e in range(epochs):\n",
        "  \n",
        "  running_loss = 0.0\n",
        "  running_corrects = 0.0\n",
        "  val_running_loss = 0.0\n",
        "  val_running_corrects = 0.0\n",
        "  \n",
        "  for inputs, labels in training_loader:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    running_loss += loss.item()\n",
        "    running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "  else:\n",
        "    with torch.no_grad():\n",
        "      for val_inputs, val_labels in validation_loader:\n",
        "        val_inputs = val_inputs.to(device)\n",
        "        val_labels = val_labels.to(device)\n",
        "        val_outputs = model(val_inputs)\n",
        "        val_loss = criterion(val_outputs, val_labels)\n",
        "\n",
        "        _, val_preds = torch.max(val_outputs, 1)\n",
        "        val_running_loss += val_loss.item()\n",
        "        val_running_corrects += torch.sum(val_preds == val_labels.data)\n",
        "      \n",
        "    epoch_loss = running_loss/len(training_loader.dataset)\n",
        "    epoch_acc = running_corrects.float()/ len(training_loader.dataset)\n",
        "    running_loss_history.append(epoch_loss)\n",
        "    running_corrects_history.append(epoch_acc)\n",
        "    \n",
        "    val_epoch_loss = val_running_loss/len(validation_loader.dataset)\n",
        "    val_epoch_acc = val_running_corrects.float()/ len(validation_loader.dataset)\n",
        "    val_running_loss_history.append(val_epoch_loss)\n",
        "    val_running_corrects_history.append(val_epoch_acc)\n",
        "    print('epoch :', (e+1))\n",
        "    print('training loss: {:.4f}, acc {:.4f} '.format(epoch_loss, epoch_acc.item()))\n",
        "    print('validation loss: {:.4f}, validation acc {:.4f} '.format(val_epoch_loss, val_epoch_acc.item()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEh89BxZC0ox"
      },
      "source": [
        "## Guardar modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "abT40udzHcFT"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(model, path):\n",
        "    # Extract the final classifier and the state dictionary\n",
        "    \n",
        "    state = {}\n",
        "\n",
        "    # Check to see if model was parallelized\n",
        "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
        "        state['classifier'] = model.module.classifier\n",
        "        state['state_dict'] = model.module.state_dict()\n",
        "\n",
        "    else:\n",
        "       state['classifier'] = model.classifier\n",
        "       state['state_dict'] = model.state_dict()\n",
        "\n",
        "    # Save the data to the path\n",
        "    torch.save(state, path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv6ijtpdHeSi"
      },
      "source": [
        "## Cargar modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GS3o_N-RHfwk"
      },
      "outputs": [],
      "source": [
        "def load_checkpoint(path, device):\n",
        "\n",
        "    # Load in checkpoint\n",
        "    state = torch.load(path, map_location=torch.device('cpu'))\n",
        "\n",
        "    # Non trainable model\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "\n",
        "    # Load in the state dict\n",
        "    model.classifier = state['classifier']\n",
        "    model.load_state_dict(state['state_dict'])\n",
        "\n",
        "    return model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tlbJgcIUC25T",
        "outputId": "db31fd40-22b6-4c9f-a6df-fa013c1cc135",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=5, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "save_checkpoint(model, 'classifier.trch')\n",
        "load_checkpoint('classifier.trch', device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "VGG-TF.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "07498890263c38a1c095232e2a086cf5383785c7f0d36a597f37f80ed3bdc2da"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}